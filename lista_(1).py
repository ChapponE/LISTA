# -*- coding: utf-8 -*-
"""LISTA_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DcvQPwRrw04dwkodKpc2N8mIh_S5DMZx

## **Pretraitement:**
"""
from src.models.lista import LISTA_model
from src.utils.math_function import PSNR
from src.utils.trainer import Trainer

"""#### Using GPU"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn import MSELoss
from torch.optim import Adam, SGD, Adadelta, Adagrad
import matplotlib.pyplot as plt
import numpy as np

torch.manual_seed(42)

# Setting device on GPU if available, else MPS (Apple M1) or CPU
if torch.cuda.is_available():
     device = torch.device('cuda') # CUDA backend for NVIDIA or AMD graphic cards
else:
    try:
        if torch.backends.mps.is_available():
            device = torch.device('mps') # MPS for Apple M# processors
        else:
            device = torch.device('cpu')
    except AttributeError:
        device = torch.device('cpu')

print('Using device:', device)
print()

# Additional info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')

"""#**Définition des données de l'experience 1 et 2:**"""

import torch
# Voir dans utils prob.py fonction measure, gen_samples et random_A


# Define A as standard Gaussian :
m = 250
n = 500
A = torch.normal(torch.zeros([m,n]), torch.tensor([1.0/m]))

  # Normalize colums to have unit l2 norm:
A = A.detach().clone().to(device)
for i in range(A[0,:].shape[0]):
  norm_column = torch.norm(A[:, i], 2)
  A[:,i] /= norm_column

  #Gram matrix and max eigenvalue:
def max_eig(A):
  gram_A = torch.matmul(A.transpose(0,1),A)
  eigenvalues, eigenvectors = torch.linalg.eig(gram_A)
  L = torch.max(torch.abs(eigenvalues))
  return(gram_A, L)

gram_A, L = max_eig(A)
L *= 1.01

# Samples definition:
SNR = 30.
def gen_samples(nb_samples, noise=False):
  X_target = torch.normal(torch.zeros([nb_samples, n]), torch.tensor([1])).to(device)
    # Samples sparse :
  eps = torch.zeros([nb_samples, m], device = device)
  pb = 0.1
  coin = torch.bernoulli(pb*torch.ones([nb_samples, n])).to(device)
  X_target = X_target*coin

  if noise:
    eps = torch.normal(torch.zeros([nb_samples, m]), torch.pow(torch.tensor(10.0), torch.tensor(-SNR/20.0))).to(device)
  # Observed value:
  X_observed = torch.matmul(X_target, A.transpose(0,1)) + eps
  return(torch.utils.data.TensorDataset(X_observed, X_target.requires_grad_(True), torch.arange(nb_samples)))

#Train data et test data:
nb_samples_train = 1000
train_data = gen_samples(nb_samples_train)
test_data = gen_samples(nb_samples_train)

# Taille des données:
var_names = ['X_observed', 'X_target', 'id']
for i, var_name in enumerate(var_names):
  print(f'{var_name}={train_data[:][i].shape}')

  #  Taille gram et L:
  print(gram_A.shape, L.shape)


'%%%%% With noise %%%%'
#Train data et test data:
nb_samples_train = 1000
train_data_noisy = gen_samples(nb_samples_train, noise = True)
test_data_noisy = gen_samples(nb_samples_train, noise = True)

#nb_samples = train_data[:][0].shape[0]
nb_samples = len(train_data)
size_target = train_data[:][1].shape[1]
size_observed = train_data[:][0].shape[1]

# Create the neural network and move it to the computational device
Lista_model = LISTA_model( size_observed=size_observed, size_target=size_target, A=A, depth=16, init=True).to(device)
Lista_model_noisy = LISTA_model( size_observed=size_observed, size_target=size_target, A=A, depth=16, init=True).to(device)
print("Number of parameters:", sum(w.numel() for w in Lista_model.parameters()))

# Create the DataLoader and configure mini-batching
batch_size = nb_samples
train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)
train_dataloader_noisy = DataLoader(train_data_noisy, batch_size=batch_size, shuffle=True)
val_dataloader_noisy = DataLoader(test_data_noisy, batch_size=batch_size, shuffle=True)

# Initialize loss, optimizer and trainer
loss = MSELoss()
# optimizer = Adam(Lista_model.parameters(), lr=1e-5)
optimizer = Adadelta(Lista_model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0, foreach=None, maximize=False)
#optimizer = Adagrad(Lista_model.parameters(), lr=0.001)
#optimizer = SGD(Lista_model.parameters(), lr=0.01, momentum=0.9)

#optimizer_noisy = Adam(Lista_model_noisy.parameters(), lr=1e-3)
optimizer_noisy = Adadelta(Lista_model_noisy.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0, foreach=None, maximize=False)
#optimizer_noisy = Adagrad(Lista_model_noisy.parameters(), lr=0.001)
#optimizer_noisy = SGD(Lista_model_noisy.parameters(), lr=0.01, momentum=0.9)

scheduler = False
#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3)
Lista_trainer = Trainer(Lista_model, optimizer, scheduler, loss,
                  train_dataloader, val_dataloader, check_val_every_n_epoch=10)
Lista_trainer_noisy = Trainer(Lista_model_noisy, optimizer_noisy, scheduler, loss,
                  train_dataloader_noisy, val_dataloader_noisy, check_val_every_n_epoch=100)
# # Run training
print(Lista_model.threshold)
Lista_trainer.run(1200)
print(Lista_model.threshold)

print(Lista_model_noisy.threshold)
Lista_trainer_noisy.run(1200)
print(Lista_model_noisy.threshold)
